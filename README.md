# Projeto 1: Plataforma de An√°lise de Streaming em Tempo Real

## üéØ Sum√°rio Executivo

Este projeto implementa um pipeline de dados de ponta a ponta para a an√°lise em tempo real de eventos de clickstream de um site de e-commerce. A solu√ß√£o ingere eventos de cliques de usu√°rios, processa-os com o Apache Spark para calcular m√©tricas de neg√≥cio em janelas de tempo e persiste os resultados em um banco de dados anal√≠tico (ClickHouse) para visualiza√ß√£o em um dashboard de BI (Metabase). O objetivo √© fornecer insights operacionais com lat√™ncia de segundos, demonstrando uma arquitetura escal√°vel, resiliente e gerenciada como c√≥digo (IaC).

## üìà O Desafio de Neg√≥cio

Uma empresa de e-commerce em crescimento precisa de visibilidade imediata sobre como os usu√°rios interagem com sua plataforma. As an√°lises baseadas em lotes (batch) existentes t√™m uma lat√™ncia de horas, o que √© muito lento para responder a tend√™ncias de navega√ß√£o, identificar sess√µes de usu√°rios com problemas ou detectar atividades fraudulentas (bots) em tempo h√°bil. A empresa necessita de uma solu√ß√£o que possa processar dezenas de milhares de eventos por segundo, garantindo ao mesmo tempo a qualidade e a integridade dos dados para a tomada de decis√£o.

## üèóÔ∏è Arquitetura da Solu√ß√£o

A arquitetura foi projetada para ser escal√°vel, resiliente e otimizada para custos, utilizando um modelo de desenvolvimento h√≠brido (local + nuvem).

> **Nota:** O diagrama final da arquitetura completa ser√° adicionado ao concluir o projeto.

### üåä Fluxo de Dados

1.  **Gera√ß√£o de Dados:** Um script Python (`data_producer`) simula eventos de clickstream e os publica no Apache Kafka.
2.  **Ingest√£o e Buffer:** O Apache Kafka atua como um *message broker* central, desacoplando produtores e consumidores.
3.  **Processamento em Stream:** Um job do Apache Spark (`stream_processor`) consome os dados, valida sua qualidade, e os agrega em janelas de tempo de 1 minuto com *watermarking* para gerenciar dados atrasados.
4.  **Persist√™ncia:** Os resultados agregados s√£o escritos via JDBC no ClickHouse, um banco de dados colunar de alta performance.
5.  **Visualiza√ß√£o:** O Metabase se conecta ao ClickHouse para exibir os dados em dashboards interativos em tempo real.

## üõ†Ô∏è Tech Stack (Pilha de Tecnologias)

| Componente | Tecnologia | Justificativa de Neg√≥cio/T√©cnica |
| :--- | :--- | :--- |
| **Gera√ß√£o de Dados** | Python (Faker, kafka-python) | Simula um ambiente de produ√ß√£o realista com dados din√¢micos. |
| **Message Broker** | Apache Kafka | Padr√£o da ind√∫stria para ingest√£o de dados em alto volume e baixa lat√™ncia. Desacopla sistemas e garante resili√™ncia. |
| **Processamento** | Apache Spark (PySpark) | Motor de processamento distribu√≠do ideal para transforma√ß√µes complexas e agrega√ß√µes em streams de dados com alta performance. |
| **Banco de Dados** | ClickHouse | Banco de dados colunar otimizado para escritas em lote e leituras anal√≠ticas (OLAP) extremamente r√°pidas, perfeito para alimentar dashboards. |
| **BI / Visualiza√ß√£o** | Metabase | Ferramenta de BI open-source que permite a cria√ß√£o r√°pida de dashboards e a explora√ß√£o de dados de forma intuitiva. |
| **Infraestrutura Local** | Docker / Docker Compose | Permite a cria√ß√£o de um ambiente de desenvolvimento local completo e reprodut√≠vel, encapsulando todas as depend√™ncias. |
| **Infraestrutura na Nuvem** | AWS (EC2, VPC, S3) | Provedor de nuvem l√≠der de mercado, oferecendo os blocos de constru√ß√£o para uma infraestrutura escal√°vel e segura. |
| **Infra como C√≥digo** | Terraform | Automatiza o provisionamento da infraestrutura na nuvem (AWS), garantindo consist√™ncia, reprodutibilidade e controle de vers√£o. |

---

## üöÄ Guia de Execu√ß√£o

Este projeto foi projetado para ser executado tanto localmente (para desenvolvimento e testes r√°pidos) quanto na nuvem da AWS (para valida√ß√£o em um ambiente de produ√ß√£o simulado).

### 1. Execu√ß√£o Local com Docker

Este modo √© ideal para desenvolver, testar e entender o fluxo de dados sem incorrer em custos de nuvem.

#### ‚úÖ Pr√©-requisitos Locais

-   Windows com WSL2, ou um sistema Linux/macOS
-   Docker e Docker Compose instalados
-   Git

#### ‚öôÔ∏è Passos para Execu√ß√£o Local

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/seu-usuario/seu-repositorio.git
    cd seu-repositorio
    ```

2.  **Permiss√£o de Execu√ß√£o (Apenas na primeira vez):**
    ```bash
    chmod +x ./clickhouse/init-db.sh
    ```

3.  **Inicie toda a infraestrutura:**
    ```bash
    docker-compose up -d
    ```

4.  **Acesse as UIs para monitoramento:**
    -   **Spark Master UI:** [http://localhost:8080](http://localhost:8080)
    -   **Metabase UI:** [http://localhost:3030](http://localhost:3030) (Note a porta 3030)

5.  **Crie o t√≥pico no Kafka:**
    ```bash
    docker exec -it kafka kafka-topics --create \
      --bootstrap-server kafka:9092 \
      --topic clickstream_events \
      --partitions 1 \
      --replication-factor 1
    ```

6.  **Inicie o produtor de dados (em um novo terminal):**
    ```bash
    python data_producer/data_producer.py
    ```

7.  **Inicie o processador de stream Spark (em outro terminal):**
    ```bash
    docker exec -it spark-master spark-submit \
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,com.clickhouse:clickhouse-jdbc:0.5.0,org.apache.httpcomponents.client5:httpclient5:5.2.1 \
      /app/spark_processor/stream_processor.py
    ```

### 2. Execu√ß√£o na Nuvem com Terraform e AWS

Este modo demonstra a capacidade de provisionar e implantar a solu√ß√£o em um ambiente de nuvem de forma automatizada, validando a arquitetura em um cen√°rio realista.

#### ‚úÖ Pr√©-requisitos da Nuvem

-   Uma conta AWS.
-   AWS CLI instalado e configurado.
-   Terraform instalado.
-   Um par de chaves SSH criado na sua conta AWS.

#### ‚öôÔ∏è Passos para Execu√ß√£o na Nuvem

1.  **Navegue at√© o diret√≥rio do Terraform:**
    ```bash
    cd terraform
    ```

2.  **Configure a Infraestrutura:**
    -   Edite o arquivo `main.tf` para configurar seu endere√ßo IP na regra de SSH do Security Group.
    -   Adicione sua chave p√∫blica SSH (extra√≠da do seu arquivo `.pem`) ao recurso `aws_key_pair`.
    -   O tipo de inst√¢ncia est√° configurado como `t3.micro` para um equil√≠brio entre performance e custo (dentro do Free Tier).

3.  **Provisione a Infraestrutura:**
    ```bash
    terraform init  # Apenas na primeira vez
    terraform apply # Para criar a infraestrutura
    ```

4.  **Acesse o Servidor:**
    -   Obtenha o IP p√∫blico da inst√¢ncia EC2 criada no console da AWS.
    -   Conecte-se via SSH:
        ```bash
        ssh -i /caminho/para/sua/chave.pem ubuntu@IP_PUBLICO_DA_INSTANCIA
        ```

5.  **Configure o Servidor e Implante a Aplica√ß√£o (Dentro do SSH):**
    -   **Instale as depend√™ncias (Git, Docker, etc.):**
        ```bash
        # Atualiza o sistema
        sudo apt-get update && sudo apt-get upgrade -y
        # Instala depend√™ncias e o Git
        sudo apt-get install -y ca-certificates curl gnupg git
        # Instala o Docker (script completo nos relat√≥rios do projeto)
        # ...
        ```
    -   **Clone o reposit√≥rio, configure permiss√µes e execute o `docker compose`** conforme os passos da execu√ß√£o local.

6.  **Acesse as UIs:**
    Ap√≥s a implanta√ß√£o, as interfaces estar√£o dispon√≠veis nos seguintes endere√ßos:
    -   **Spark Master UI:** `http://<IP_PUBLICO_DA_INSTANCIA>:8080`
    -   **Metabase UI:** `http://<IP_PUBLICO_DA_INSTANCIA>:3030`

7.  **Desprovisione a Infraestrutura:**
    **IMPORTANTE:** Para evitar custos, destrua toda a infraestrutura ap√≥s a valida√ß√£o.
    ```bash
    # No diret√≥rio terraform, na sua m√°quina local
    terraform destroy
    ```

---

## üí° Li√ß√µes Aprendidas e Pr√≥ximas Etapas

*   **Dimensionamento de Inst√¢ncias (Right-Sizing):** A implanta√ß√£o inicial em uma inst√¢ncia `t2.micro` falhou devido √† sobrecarga de recursos durante a inicializa√ß√£o simult√¢nea de m√∫ltiplos servi√ßos. A migra√ß√£o para uma `t3.micro`, que possui um modelo de cr√©ditos de CPU mais flex√≠vel, resolveu o problema de responsividade. Isso destaca a import√¢ncia de escolher o tipo de inst√¢ncia adequado para a carga de trabalho, mesmo em fases de teste.
*   **Ciclo de Vida de IaC:** O projeto demonstrou o ciclo de vida completo da Infraestrutura como C√≥digo: provisionamento (`apply`), modifica√ß√£o (mudan√ßa do tipo de inst√¢ncia) e desprovisionamento (`destroy`), tudo gerenciado de forma controlada e previs√≠vel.
*   **Depura√ß√£o Multi-camada:** Os desafios enfrentados exigiram depura√ß√£o em todas as camadas da stack: c√≥digo Terraform, configura√ß√µes da AWS (Security Groups, AZs), sistema operacional do servidor (UFW) e a pr√≥pria aplica√ß√£o (Docker).

**Pr√≥ximas Etapas (Para um Projeto em Produ√ß√£o):**
1.  **Utilizar Servi√ßos Gerenciados:** Migrar de servi√ßos auto-hospedados em EC2 para servi√ßos gerenciados da AWS (ex: Amazon MSK para Kafka, Amazon EMR para Spark, Amazon RDS/Redshift para o banco de dados) para aumentar a resili√™ncia e reduzir a sobrecarga operacional.
2.  **Automa√ß√£o de CI/CD:** Criar um pipeline de CI/CD (ex: com GitHub Actions) para automatizar os testes e a implanta√ß√£o da aplica√ß√£o sempre que houver uma altera√ß√£o no c√≥digo.
3.  **Monitoramento e Alertas:** Implementar uma solu√ß√£o de monitoramento robusta (ex: Prometheus/Grafana ou Amazon CloudWatch) para observar a sa√∫de da aplica√ß√£o e da infraestrutura, com alertas para falhas ou anomalias.

---

---

## üìä Resultados e Valor de Neg√≥cio

O pipeline foi validado com sucesso, culminando em um dashboard no Metabase que exibe a contagem de cliques por p√°gina em tempo real.

![Dashboard Final com Dados Realistas](/jpeg/graf2.jpg)

**An√°lise de Valor:**
*   **Visibilidade Operacional:** Stakeholders podem monitorar o engajamento das p√°ginas no √∫ltimo minuto.
*   **Tomada de Decis√£o R√°pida:** Anomalias, como uma queda dr√°stica de cliques em p√°ginas cr√≠ticas (ex: `/cart`), podem ser detectadas em segundos.
*   **Valida√ß√£o de Arquitetura:** O gr√°fico prova que a arquitetura √© capaz de ingerir, processar, agregar e visualizar dados com baixa lat√™ncia, cumprindo o objetivo principal do projeto.